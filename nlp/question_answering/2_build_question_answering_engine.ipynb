{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e410b6c4",
   "metadata": {},
   "source": [
    "# Build a Qusetion Answering Engine in Minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bed6f24",
   "metadata": {},
   "source": [
    "This notebook illustrates how to build a question answering engine from scratch using [Milvus](https://milvus.io/) and [Towhee](https://towhee.io/). Milvus is the most advanced open-source vector database built for AI applications and supports nearest neighbor embedding search across tens of millions of entries, and Towhee is a framework that provides ETL for unstructured data using SoTA machine learning models.\n",
    "\n",
    "We will go through question answering procedures and evaluate performance. Moreover, we managed to make the core functionality as simple as almost 10 lines of code with Towhee, so that you can start hacking your own question answering engine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4883e577",
   "metadata": {},
   "source": [
    "## Preparations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49110b91",
   "metadata": {},
   "source": [
    "### Install Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0117995a",
   "metadata": {},
   "source": [
    "First we need to install dependencies such as towhee, towhee.models and gradio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9ba3850",
   "metadata": {},
   "outputs": [],
   "source": [
    "! python -m pip install -q towhee towhee.models gradio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90db0c5",
   "metadata": {},
   "source": [
    "### Prepare the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1eceb58",
   "metadata": {},
   "source": [
    "There is a subset of the  [InsuranceQA Corpus](https://github.com/shuzi/insuranceQA)  (1000 pairs of questions and answers) used in this demo, everyone can download on [Github](https://github.com/towhee-io/examples/releases/download/data/question_answer.csv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1436a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "! curl -L https://github.com/towhee-io/examples/releases/download/data/question_answer.csv -O"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4abdc0a",
   "metadata": {},
   "source": [
    "**question_answer.csv**: a file containing question and the answer.\n",
    "\n",
    "Let's take a quick look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d652efea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('question_answer.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309bfb43",
   "metadata": {},
   "source": [
    "To use the dataset to get answers, let's first define the dictionary:\n",
    "\n",
    "- `id_answer`: a dictionary of id and corresponding answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d98b309",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_answer = df.set_index('id')['answer'].to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5a0858",
   "metadata": {},
   "source": [
    "### Create Milvus Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb06a01",
   "metadata": {},
   "source": [
    "Before getting started, please make sure that you have started a [Milvus service](https://milvus.io/docs/install_standalone-docker.md). This notebook uses [milvus 2.2.10](https://milvus.io/docs/v2.2.x/install_standalone-docker.md) and [pymilvus 2.2.11](https://milvus.io/docs/release_notes.md#2210)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8048bf6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "! python -m pip install -q pymilvus==2.2.11\n",
    "! python -m pip install -q setuptools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ba2b23",
   "metadata": {},
   "source": [
    "Next to define the function `create_milvus_collection` to create collection in Milvus that uses the [L2 distance metric](https://milvus.io/docs/metric.md#Euclidean-distance-L2) and an [IVF_FLAT index](https://milvus.io/docs/index.md#IVF_FLAT)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22c19982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'> 77e44c13012a64a48c21312903bf753be640225497971e1d1d1c13fe4fcec0cc0b4fa70b30aa2caba9e2fec9d38379eb853b9dc9\n"
     ]
    }
   ],
   "source": [
    "from pymilvus import connections, FieldSchema, CollectionSchema, DataType, Collection, utility, MilvusClient\n",
    "import configparser\n",
    "\"\"\"\n",
    "client = MilvusClient(\n",
    "\n",
    "uri= '',\n",
    "\n",
    "token= ''\n",
    "\n",
    ")\"\"\"\n",
    "\n",
    "cfp = configparser.RawConfigParser()\n",
    "cfp.read('settings.ini')\n",
    "print(type(cfp.get(\"settings\",\"uri\")), cfp.get(\"settings\",\"token\"))\n",
    "connections.connect(\n",
    "    uri= cfp.get(\"settings\",\"uri\"),\n",
    "    token= cfp.get(\"settings\",\"token\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a44683a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import connections, FieldSchema, CollectionSchema, DataType, Collection, utility, MilvusClient\n",
    "import configparser\n",
    "\"\"\" # Create a collection\n",
    "client.create_collection(\n",
    "    collection_name='testuser2',\n",
    "    dimension=768\n",
    ")\n",
    " \"\"\"\n",
    "def create_milvus_collection(collection_name, dim):\n",
    "    if utility.has_collection(collection_name):\n",
    "        utility.drop_collection(collection_name)\n",
    "    \n",
    "    fields = [\n",
    "    FieldSchema(name='id', dtype=DataType.VARCHAR, descrition='ids', max_length=500, is_primary=True, auto_id=False),\n",
    "    FieldSchema(name='embedding', dtype=DataType.FLOAT_VECTOR, descrition='embedding vectors', dim=dim)\n",
    "    ]\n",
    "    schema = CollectionSchema(fields=fields, description='reverse image search')\n",
    "    collection = Collection(name=collection_name, schema=schema)\n",
    "\n",
    "    # create IVF_FLAT index for collection.\n",
    "    index_params = {\n",
    "        'metric_type':'L2',\n",
    "        'index_type':\"IVF_FLAT\",\n",
    "        'params':{\"nlist\":2048}\n",
    "    }\n",
    "    collection.create_index(field_name=\"embedding\", index_params=index_params)\n",
    "    return collection\n",
    "\n",
    "collection = create_milvus_collection('testuser2', 768)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9724ba28",
   "metadata": {},
   "source": [
    "## Question Answering Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e1ac7e",
   "metadata": {},
   "source": [
    "In this section, we will show how to build our question answering engine using Milvus and Towhee. The basic idea behind question answering is to use Towhee to generate embedding from the question dataset and compare the input question with the embedding stored in Milvus.\n",
    "\n",
    "[Towhee](https://towhee.io/) is a machine learning framework that allows the creation of data processing pipelines, and it also provides predefined operators for implementing insert and query operations in Milvus.\n",
    "\n",
    "<img src=\"./workflow.png\" width = \"60%\" height = \"60%\" align=center />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0188bf",
   "metadata": {},
   "source": [
    "### Load question embedding into Milvus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a654fdc",
   "metadata": {},
   "source": [
    "We first generate embedding from question text with [dpr](https://towhee.io/text-embedding/dpr) operator and insert the embedding into Milvus. Towhee provides a [method-chaining style API](https://towhee.readthedocs.io/en/main/index.html) so that users can assemble a data processing pipeline with operators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9837e273",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-14 19:35:17,637 - 139859133390912 - connectionpool.py-connectionpool:549 - DEBUG: https://huggingface.co:443 \"HEAD /facebook/dpr-ctx_encoder-single-nq-base/resolve/main/tokenizer_config.json HTTP/1.1\" 200 0\n",
      "2024-05-14 19:35:17,871 - 139859133390912 - connectionpool.py-connectionpool:549 - DEBUG: https://huggingface.co:443 \"HEAD /facebook/dpr-ctx_encoder-single-nq-base/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "2024-05-14 19:35:18,035 - 139859133390912 - connectionpool.py-connectionpool:549 - DEBUG: https://huggingface.co:443 \"HEAD /facebook/dpr-ctx_encoder-single-nq-base/resolve/main/config.json HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/student/PycharmProjects/MagProject/project3/ProjectOnTeam/nlp/question_answering/2_build_question_answering_engine.ipynb Cell 24\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/student/PycharmProjects/MagProject/project3/ProjectOnTeam/nlp/question_answering/2_build_question_answering_engine.ipynb#X32sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m text2 \u001b[39m=\u001b[39m base64\u001b[39m.\u001b[39mb64decode(src)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/student/PycharmProjects/MagProject/project3/ProjectOnTeam/nlp/question_answering/2_build_question_answering_engine.ipynb#X32sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m decompressed_data\u001b[39m=\u001b[39mzlib\u001b[39m.\u001b[39mdecompress(text2, \u001b[39m16\u001b[39m\u001b[39m+\u001b[39mzlib\u001b[39m.\u001b[39mMAX_WBITS)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/student/PycharmProjects/MagProject/project3/ProjectOnTeam/nlp/question_answering/2_build_question_answering_engine.ipynb#X32sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m insert_pipe([txtId, decompressed_data, txtId])\t\n",
      "File \u001b[0;32m~/PycharmProjects/MagProject/project3/ProjectOnTeam/.venv/lib/python3.11/site-packages/towhee/runtime/runtime_pipeline.py:159\u001b[0m, in \u001b[0;36mRuntimePipeline.__call__\u001b[0;34m(self, *inputs)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39minputs):\n\u001b[1;32m    156\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \u001b[39m    Output with ordering matching the input `DataQueue`.\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 159\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49minputs, profiler\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, tracer\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/PycharmProjects/MagProject/project3/ProjectOnTeam/.venv/lib/python3.11/site-packages/towhee/runtime/runtime_pipeline.py:177\u001b[0m, in \u001b[0;36mRuntimePipeline._call\u001b[0;34m(self, profiler, tracer, trace_edges, *inputs)\u001b[0m\n\u001b[1;32m    174\u001b[0m time_profiler \u001b[39m=\u001b[39m TimeProfiler(\u001b[39mTrue\u001b[39;00m) \u001b[39mif\u001b[39;00m profiler \u001b[39melse\u001b[39;00m TimeProfiler(\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    175\u001b[0m graph \u001b[39m=\u001b[39m _Graph(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dag_repr\u001b[39m.\u001b[39mnodes, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dag_repr\u001b[39m.\u001b[39medges, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_operator_pool, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_thread_pool, time_profiler, trace_edges)\n\u001b[0;32m--> 177\u001b[0m \u001b[39mreturn\u001b[39;00m graph(inputs), [graph\u001b[39m.\u001b[39mtime_profiler] \u001b[39mif\u001b[39;00m profiler \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m, [graph\u001b[39m.\u001b[39mdata_queues] \u001b[39mif\u001b[39;00m tracer \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/PycharmProjects/MagProject/project3/ProjectOnTeam/.venv/lib/python3.11/site-packages/towhee/runtime/runtime_pipeline.py:115\u001b[0m, in \u001b[0;36m_Graph.__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, inputs: Union[Tuple, List]):\n\u001b[0;32m--> 115\u001b[0m     f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49masync_call(inputs)\n\u001b[1;32m    116\u001b[0m     \u001b[39mreturn\u001b[39;00m f\u001b[39m.\u001b[39mresult()\n",
      "File \u001b[0;32m~/PycharmProjects/MagProject/project3/ProjectOnTeam/.venv/lib/python3.11/site-packages/towhee/runtime/runtime_pipeline.py:103\u001b[0m, in \u001b[0;36m_Graph.async_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39masync_call\u001b[39m(\u001b[39mself\u001b[39m, inputs: Union[Tuple, List]):\n\u001b[1;32m    102\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtime_profiler\u001b[39m.\u001b[39minputs \u001b[39m=\u001b[39m inputs\n\u001b[0;32m--> 103\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_input_queue\u001b[39m.\u001b[39;49mput(inputs)\n\u001b[1;32m    104\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_input_queue\u001b[39m.\u001b[39mseal()\n\u001b[1;32m    105\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeatures \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/PycharmProjects/MagProject/project3/ProjectOnTeam/.venv/lib/python3.11/site-packages/towhee/runtime/data_queue.py:51\u001b[0m, in \u001b[0;36mDataQueue.put\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mput\u001b[39m(\u001b[39mself\u001b[39m, inputs: Union[Tuple, List]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mbool\u001b[39m:\n\u001b[0;32m---> 51\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(inputs) \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_schema\u001b[39m.\u001b[39msize()\n\u001b[1;32m     52\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_not_full:\n\u001b[1;32m     53\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sealed:\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from base64 import b64decode\n",
    "import requests\n",
    "import base64\n",
    "import zlib\n",
    "import urllib\n",
    "\n",
    "from towhee import pipe, ops\n",
    "import numpy as np\n",
    "from towhee.datacollection import DataCollection\n",
    "\n",
    "insert_pipe = (\n",
    "    pipe.input('id', 'question', 'answer')\n",
    "        .map('question', 'vec', ops.text_embedding.dpr(model_name='facebook/dpr-ctx_encoder-single-nq-base'))\n",
    "        .map('vec', 'vec', lambda x: x / np.linalg.norm(x, axis=0))\n",
    "        .map(('id', 'vec'), 'insert_status', ops.ann_insert.milvus_client(uri=cfp.get(\"settings\",\"uri\"), token=cfp.get(\"settings\",\"token\"), collection_name='testuser2'))\n",
    "        .output()\n",
    ")\n",
    "\n",
    "txt = open('5000.tab', 'r')\n",
    "for i in range(5):\n",
    "    line = txt.readline()\n",
    "    txtId = line.split(\"\\t\")[0]\n",
    "    src = line.split(\"\\t\")[1].replace(\"\\n\",\"\")\n",
    "    if len(src) < 1000:\n",
    "        continue\n",
    "    text2 = base64.b64decode(src)\n",
    "    decompressed_data=zlib.decompress(text2, 16+zlib.MAX_WBITS)\n",
    "    insert_pipe([txtId, decompressed_data, txtId])\t\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b7beea",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from towhee import pipe, ops\n",
    "import numpy as np\n",
    "from towhee.datacollection import DataCollection\n",
    "\n",
    "insert_pipe = (\n",
    "    pipe.input('id', 'question', 'answer')\n",
    "        .map('question', 'vec', ops.text_embedding.dpr(model_name='facebook/dpr-ctx_encoder-single-nq-base'))\n",
    "        .map('vec', 'vec', lambda x: x / np.linalg.norm(x, axis=0))\n",
    "        .map(('id', 'vec'), 'insert_status', ops.ann_insert.milvus_client(uri=cfp.get(\"settings\",\"uri\"), token=cfp.get(\"settings\",\"token\"), collection_name='testuser2'))\n",
    "        .output()\n",
    ")\n",
    "\n",
    "import csv\n",
    "with open('question_answer.csv', encoding='utf-8') as f:\n",
    "    reader = csv.reader(f)\n",
    "    next(reader)\n",
    "    for row in reader:\n",
    "        insert_pipe(*row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fcecb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('question_answer.csv', encoding='utf-8') as f:\n",
    "    reader = csv.reader(f)\n",
    "    next(reader)\n",
    "    for row in reader:\n",
    "        print(*row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9db78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1adbb2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total number of inserted data is {}.'.format(collection.num_entities))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb269f4",
   "metadata": {},
   "source": [
    "#### Explanation of Data Processing Pipeline\n",
    "\n",
    "Here is detailed explanation for each line of the code:\n",
    "\n",
    "`pipe.input('id', 'question', 'answer')`: Get three inputs, namely question's id, quesion's text and question's answer;\n",
    "\n",
    "`map('question', 'vec', ops.text_embedding.dpr(model_name='facebook/dpr-ctx_encoder-single-nq-base'))`: Use the `acebook/dpr-ctx_encoder-single-nq-base` model to generate the question embedding vector with the [dpr operator](https://towhee.io/text-embedding/dpr) in towhee hub;\n",
    "\n",
    "`map('vec', 'vec', lambda x: x / np.linalg.norm(x, axis=0))`: normalize the embedding vector;\n",
    "\n",
    "`map(('id', 'vec'), 'insert_status', ops.ann_insert.milvus_client(host='127.0.0.1', port='19530', collection_name='question_answer'))`: insert question embedding vector into Milvus;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35657d0",
   "metadata": {},
   "source": [
    "### Ask Question with Milvus and Towhee"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd02adfc",
   "metadata": {},
   "source": [
    "Now that embedding for question dataset have been inserted into Milvus, we can ask question with Milvus and Towhee. Again, we use Towhee to load the input question, compute a embedding, and use it as a query in Milvus. Because Milvus only outputs IDs and distance values, we provide the `id_answers` dictionary to get the answers based on IDs and display."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95913f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "collection.load()\n",
    "ans_pipe = (\n",
    "    pipe.input('question')\n",
    "        .map('question', 'vec', ops.text_embedding.dpr(model_name=\"facebook/dpr-ctx_encoder-single-nq-base\"))\n",
    "        .map('vec', 'vec', lambda x: x / np.linalg.norm(x, axis=0))\n",
    "        .map('vec', 'res', ops.ann_search.milvus_client(uri=cfp.get(\"settings\",\"uri\"), token=cfp.get(\"settings\",\"token\"), collection_name='testuser2', limit=1))\n",
    "        .map('res', 'answer', lambda x: [id_answer[int(i[0])] for i in x])\n",
    "        .output('question', 'answer')\n",
    ")\n",
    "\n",
    "\n",
    "ans = ans_pipe('Is  Disability  Insurance  Required  By  Law?')\n",
    "ans = DataCollection(ans)\n",
    "ans.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb05a79",
   "metadata": {},
   "source": [
    "Then we can get the answer about 'Is  Disability  Insurance  Required  By  Law?'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1a8f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans[0]['answer']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01bef722",
   "metadata": {},
   "source": [
    "## Release a Showcase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71cace8",
   "metadata": {},
   "source": [
    "We've done an excellent job on the core functionality of our question answering engine. Now it's time to build a showcase with interface. [Gradio](https://gradio.app/) is a great tool for building demos. With Gradio, we simply need to wrap the data processing pipeline via a `chat` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d42114",
   "metadata": {},
   "outputs": [],
   "source": [
    "import towhee\n",
    "def chat(message, history):\n",
    "    history = history or []\n",
    "    ans_pipe = (\n",
    "        pipe.input('question')\n",
    "            .map('question', 'vec', ops.text_embedding.dpr(model_name=\"facebook/dpr-ctx_encoder-single-nq-base\"))\n",
    "            .map('vec', 'vec', lambda x: x / np.linalg.norm(x, axis=0))\n",
    "            .map('vec', 'res', ops.ann_search.milvus_client(uri=cfp.get(\"settings\",\"uri\"), token=cfp.get(\"settings\",\"token\"), collection_name='testuser2', limit=1))\n",
    "            .map('res', 'answer', lambda x: [id_answer[int(i[0])] for i in x])\n",
    "            .output('question', 'answer')\n",
    "    )\n",
    "\n",
    "    response = ans_pipe(message).get()[1][0]\n",
    "    history.append((message, response))\n",
    "    return history, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065523a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio\n",
    "\n",
    "collection.load()\n",
    "chatbot = gradio.Chatbot(color_map=(\"green\", \"gray\"))\n",
    "interface = gradio.Interface(\n",
    "    chat,\n",
    "    [\"text\", \"state\"],\n",
    "    [chatbot, \"state\"],\n",
    "    allow_screenshot=False,\n",
    "    allow_flagging=\"never\",\n",
    ")\n",
    "interface.launch(inline=True, share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23806967",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "f7dd10cdbe9a9c71f7e71741efd428241b5f9fa0fecdd29ae07a5706cd5ff8a2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
